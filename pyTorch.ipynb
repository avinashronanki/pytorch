{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP55agNxnlgQnL3i4tX0e8K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d955e561c4e6415392ca5ac174c960f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_82e1766955cc48628807032a252b911d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_440b9983388744ffb4a36209789be3f1",
              "IPY_MODEL_e6b285f0f93b4f2da75529c3348c801c"
            ]
          }
        },
        "82e1766955cc48628807032a252b911d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "440b9983388744ffb4a36209789be3f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ac06841afe2a4d408a40b36575523422",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0fb038919eac4d74a76fbd89efa1b172"
          }
        },
        "e6b285f0f93b4f2da75529c3348c801c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e6a15f32a36f4d668d4bde961cc50d40",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 144MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4d537bf49f7a4f6ab2f76368ee1432d3"
          }
        },
        "ac06841afe2a4d408a40b36575523422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0fb038919eac4d74a76fbd89efa1b172": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e6a15f32a36f4d668d4bde961cc50d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4d537bf49f7a4f6ab2f76368ee1432d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avinashronanki/pytorch/blob/master/pyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_SRKjqyvZCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "x = torch.rand(3)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHK-LzxxyMQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.is_available().  # to check if gpu is connected or not "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1l7mMB8y5Li",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "empty = torch.empty(3) # un intialized memory /data \n",
        "print(empty)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYULsopSzUqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zero = torch.zeros(3)\n",
        "print(zero)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UifAChtezhsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "empty + one"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txLjGDy-zy1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one = torch.ones(3)\n",
        "print(one)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgUr2ER10H1g",
        "colab_type": "text"
      },
      "source": [
        "create 2d 3d and 4d tensor "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8ldNM2V0M44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twod = torch.rand(2,2)\n",
        "threed = torch.rand(2,2,2)\n",
        "fourd = torch.rand(2,2,2,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEMzRphO1QJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c  = torch.ones(2,2, dtype =torch.float) # mention date type "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzCiH6in1IYw",
        "colab_type": "text"
      },
      "source": [
        "list "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQHd6D4t05Kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = torch.ones([1,2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLADFK2E0p8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfajHdpw2E_m",
        "colab_type": "text"
      },
      "source": [
        "Adding two tensors "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCMa8gb12Qu1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6ef55555-d942-4b97-ce8c-83244923ab92"
      },
      "source": [
        "x = torch.rand(2,2)\n",
        "y = torch.rand(2,2)\n",
        "\n",
        "z= x+y   \n",
        "print(z)\n",
        "\n",
        "z = torch.add(x,y) # add function \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.8325, 1.3713],\n",
            "        [0.8632, 0.8652]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj7T32b33Jg0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "84d31f96-9f43-4e19-dca0-52814694305e"
      },
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.6337, 0.7274],\n",
            "        [0.4152, 0.3477]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du3s5wfT28W4",
        "colab_type": "text"
      },
      "source": [
        "modify the variable with addition "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPD0_DAU27Tq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9e924d51-77ac-4aec-8ffa-1afec476bd45"
      },
      "source": [
        "y.add_(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.8325, 1.3713],\n",
            "        [0.8632, 0.8652]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Poa00jB3ZpB",
        "colab_type": "text"
      },
      "source": [
        "substraction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CYTyCXn3cpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.rand(2,2)\n",
        "y = torch.rand(2,2)\n",
        "\n",
        "z= x-y   \n",
        "print(z)\n",
        "\n",
        "z = torch.sub(x,y) # substraction function "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXeWLce-3jAf",
        "colab_type": "text"
      },
      "source": [
        "divison "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3ABj2Y23lyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.rand(2,2)\n",
        "y = torch.rand(2,2)\n",
        "\n",
        "z= x/ y   \n",
        "print(z)\n",
        "\n",
        "z = torch.div(x,y) # division  function "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be7UguIL6BKU",
        "colab_type": "text"
      },
      "source": [
        "Slice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFcx2xtJ4Aj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.rand(5,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ-C0xxF4YOU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "45e764d0-c169-41fd-cf70-c0854f957b82"
      },
      "source": [
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.9725, 0.8286, 0.1985, 0.6716],\n",
            "        [0.5158, 0.3561, 0.6893, 0.1104],\n",
            "        [0.8168, 0.3879, 0.8921, 0.6888],\n",
            "        [0.7019, 0.3297, 0.7578, 0.1883],\n",
            "        [0.5933, 0.2272, 0.2099, 0.8052]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSQfUoj_4NyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(a[1:3,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9YSE0nT4z6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(a[[2,4],:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPhhhiX85UpW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9fe75fb7-e9a8-4dd4-b400-ea008466df98"
      },
      "source": [
        "a[1,1] # taking a specific postion "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.3561)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXmU8GKg5k22",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "464cb33c-6295-4b8d-a68b-ea06e5c67138"
      },
      "source": [
        "a[1,1].item()   # if you need the value of the tensor "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3561347723007202"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXRsSn_b6gE9",
        "colab_type": "text"
      },
      "source": [
        "REshape "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OCYon8_6jKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "885b861a-c824-402b-a7cf-dba27b150e76"
      },
      "source": [
        "r = torch.rand(6,6)\n",
        "print (r)\n",
        "y = r.view(36) # in a single list \n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.9520, 0.2428, 0.4987, 0.7993, 0.8776, 0.7078],\n",
            "        [0.9136, 0.2007, 0.8112, 0.7529, 0.3412, 0.7893],\n",
            "        [0.9817, 0.1569, 0.0114, 0.0199, 0.5911, 0.3896],\n",
            "        [0.7242, 0.4373, 0.9833, 0.1772, 0.8281, 0.1766],\n",
            "        [0.5861, 0.9514, 0.2313, 0.0517, 0.6457, 0.7085],\n",
            "        [0.0889, 0.1592, 0.9855, 0.2104, 0.8430, 0.4124]])\n",
            "tensor([0.9520, 0.2428, 0.4987, 0.7993, 0.8776, 0.7078, 0.9136, 0.2007, 0.8112,\n",
            "        0.7529, 0.3412, 0.7893, 0.9817, 0.1569, 0.0114, 0.0199, 0.5911, 0.3896,\n",
            "        0.7242, 0.4373, 0.9833, 0.1772, 0.8281, 0.1766, 0.5861, 0.9514, 0.2313,\n",
            "        0.0517, 0.6457, 0.7085, 0.0889, 0.1592, 0.9855, 0.2104, 0.8430, 0.4124])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZdfhaXj6-Rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert into 8 column metric \n",
        "\n",
        "y = r.view(-1, 9)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51dtxeye7bne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = r.view(2, -1)\n",
        "print(z.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwnmqSNI8Ijn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z.view(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "524zXeKg8pHS",
        "colab_type": "text"
      },
      "source": [
        "numpy array "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgqvMpXi8JGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7ee2b257-4e51-422a-ebac-6a6a4e9f957e"
      },
      "source": [
        "import numpy as np\n",
        " \n",
        "a = torch.ones(5)\n",
        "print(a)\n",
        "b = a.numpy()\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n",
            "[1. 1. 1. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4WzOiuo9dbD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a91f2d4a-5d18-44fb-e256-8a120d005faf"
      },
      "source": [
        "a.add_(1)\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2., 2., 2., 2., 2.])\n",
            "[2. 2. 2. 2. 2.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHylJra497Wz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c3fcb799-4557-4f38-c29d-4de02c7961b8"
      },
      "source": [
        "# numpy to tensor \n",
        "\n",
        "n = np.ones(5)\n",
        "print(n)\n",
        "t = torch.from_numpy(n)\n",
        "print(t)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 1. 1. 1.]\n",
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgTGYd2G-acI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "950b665f-5cff-48ff-87ee-0700b5ad27ab"
      },
      "source": [
        "n += 1\n",
        "print(n)\n",
        "print(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2. 2. 2. 2. 2.]\n",
            "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81ysMNFf-xS4",
        "colab_type": "text"
      },
      "source": [
        "cuda "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBapYanw-1MI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d06b88a5-d8b7-4634-8722-4e732c0513cf"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  x = torch.ones(5, device= device)\n",
        "  y = torch.ones(5)\n",
        "  y = y.to(device) # converting into cuda \n",
        "  z= x+y \n",
        "\n",
        "  print(z)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2., 2., 2., 2., 2.], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9mZyrg6ABbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# moving it back to cpu version \n",
        "z= z.to(\"cpu\") # important \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfO8vvgjAswn",
        "colab_type": "text"
      },
      "source": [
        "Gradients calculation with Autogred "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQR74YAyA2IX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f53b37dc-2716-4300-8fac-097e811d1b7f"
      },
      "source": [
        "g = torch.ones(5, requires_grad=True)\n",
        "print(g)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdWp7riZBOIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ba857643-9cd7-4859-a0ec-d772f5656d62"
      },
      "source": [
        "x = torch.rand(3,3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "y = x+2\n",
        "print (y) #  grad_fn=<AddBackward0> "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.4301, 0.7209, 0.7149],\n",
            "        [0.2627, 0.6174, 0.3840],\n",
            "        [0.8414, 0.7427, 0.7175]], requires_grad=True)\n",
            "tensor([[2.4301, 2.7209, 2.7149],\n",
            "        [2.2627, 2.6174, 2.3840],\n",
            "        [2.8414, 2.7427, 2.7175]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFPNuJHeCQsV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "53043ce8-b6ed-4bd3-b6db-fbd1c74c883a"
      },
      "source": [
        "z = y*y*2\n",
        "\n",
        "print (z) #grad_fn=<MulBackward0>)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[11.8104, 14.8063, 14.7417],\n",
            "        [10.2396, 13.7019, 11.3670],\n",
            "        [16.1467, 15.0444, 14.7700]], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGA5gQ_rCe2i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "05711fea-2719-44cc-fe9f-81d804ba4040"
      },
      "source": [
        "z = z.mean()\n",
        "print(z) # grad_fn=<MeanBackward0>)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(13.6253, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ax2y6BYCt2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "47e5017e-a82c-4342-f4d2-7c9115a34e94"
      },
      "source": [
        "# backward pass \n",
        "z.backward() # dz/dx\n",
        "print (x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.0800, 1.2093, 1.2066],\n",
            "        [1.0056, 1.1633, 1.0596],\n",
            "        [1.2628, 1.2190, 1.2078]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6ESW2WtEEyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with out disturbing the gradents perfroming some operation on x \n",
        "x = torch.rand(3, requires_grad=True)\n",
        "print(x)\n",
        "x.requires_grad(False)\n",
        "x.detach()\n",
        "with torch.no_grad()\n",
        "\n",
        "y = X+2 \n",
        "print (y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vunTeplWGwkc",
        "colab_type": "text"
      },
      "source": [
        "Neural network with weights "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzR04wB0HBq6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "11463a68-58e9-4b7a-97f7-cb0f291a666b"
      },
      "source": [
        "import torch \n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "print(weights)\n",
        "for epoch in range(3):\n",
        "  model_output = (weights*3).sum()\n",
        "  model_output.backward()\n",
        "  print(weights.grad)\n",
        "  weights.grad.zero_()\n",
        "  print(model_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1., 1.], requires_grad=True)\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor(12., grad_fn=<SumBackward0>)\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor(12., grad_fn=<SumBackward0>)\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor(12., grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDtic0WxJSdT",
        "colab_type": "text"
      },
      "source": [
        "optimizer SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL4IOj9OJX69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "optimizer = torch.optim.SGD(weights, lr= 0.01)\n",
        "optimizer.step()\n",
        "optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYNmqnJxKjp4",
        "colab_type": "text"
      },
      "source": [
        "Backpropagation \n",
        "1. forward pass (loss)\n",
        "2.local gradients \n",
        "3.backword propagation loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoirCyGwKn0m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1b3191ab-c277-4843-a358-f27668337255"
      },
      "source": [
        "# chain rule \n",
        "# computational graph \n",
        "\n",
        "#linear regression \n",
        "\n",
        "\n",
        "import torch \n",
        "\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "w= torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "#forword pass and compute loss\n",
        "\n",
        "y_hat = w * x\n",
        "loss =(y_hat - y)**2\n",
        "\n",
        "print(loss)\n",
        "\n",
        "#backword pass \n",
        "loss.backward()\n",
        "print(w.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCIW9uwLTMKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edU1m15vUH4x",
        "colab_type": "text"
      },
      "source": [
        "Gradient Descent With Autograd and Backpropagation \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7aL9en3UUxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction : pytoch model \n",
        "GD : Auto grad \n",
        "Loss Computation : loss function \n",
        "parameter : optimizer "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsNZ4AQvbwH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#linear Regression \n",
        "\n",
        "X = np.array([1,2,3,4], dtype=np.float32)\n",
        "Y = np.array([2,4,6,8], dtype=np.float32)\n",
        "w=0.0\n",
        "\n",
        "\n",
        "#model prediction \n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "\n",
        "#loss =MSE\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted - y)**2).mean()\n",
        "\n",
        "\n",
        "#gardient\n",
        "#MSE = 1/N * (w*x -y) **2\n",
        "#d/dw =1/N 2x (w*x -y)\n",
        "def gradient( x, y, y_predicted):\n",
        "  return np.dot(2*x, y_predicted -y).mean()\n",
        "\n",
        "print(\"Prediction before training: f(5) = {forward(5):.3f}\")\n",
        "\n",
        "#training\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  #loss\n",
        "  l = loss( Y, y_pred)\n",
        "\n",
        "\n",
        "  #gradients\n",
        "  dw = gradient( X, Y, y_pred)\n",
        "\n",
        "\n",
        " #update weights\n",
        "  w -=learning_rate * dw\n",
        "\n",
        "  if epoch % 2 == 0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss {l:.8f}')\n",
        "\n",
        "print(f\"Prediction after training: f(5) = {forward(5):.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4ILPkXucmTz",
        "colab_type": "text"
      },
      "source": [
        "Liner regression with pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdBqMjW_cs0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "#needs an input size and output size as features .. need to be a 2D array .. no of rows is the no of samples so change the shape of X & Y \n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32) \n",
        "\n",
        "\n",
        "n_samples, n_features =X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "#remove the weights since the function nn.linear() contains the weights inbuilt linear regression function by pytorch\n",
        "#model prediction \n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "# foward == model \n",
        "\n",
        "# weigths become models.paramters  // used by DeClare\n",
        "#linear model with random weigths will be created\n",
        "\n",
        "\n",
        "#creating a custom model ( where many layers can be added for fine tuning)\n",
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "      super(LinearRegression, self).__init__()   # super constructer\n",
        "      #define layers here \n",
        "      self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "     return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "print(\"Prediction before training: f(5) = {model(X_test).item():.3f}\")\n",
        "\n",
        "# remove the loss function and add the built in function MSELoss()\n",
        "\n",
        "#training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "\n",
        "# acts as a back propogation\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  #prediction = forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  #loss\n",
        "  l = loss( Y, y_pred)\n",
        "\n",
        "\n",
        "  #gradients = backward pass\n",
        "  l.backward() #loss with respect to w --> dl/dw\n",
        "  \n",
        "  # dw = gradient( X, Y, y_pred)\n",
        "\n",
        "\n",
        "  #update weights\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "  #zero gradients\n",
        "  optimizer.zero_grad()\n",
        "  if epoch % 10 == 0:\n",
        "    # wanrt to print so unpack them \n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss {l:.8f}')\n",
        "\n",
        "print(f\"Prediction after training: f(5) = {model(X_test).item():.3f}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-dPBsNvTxi1",
        "colab_type": "text"
      },
      "source": [
        "# 1) Design model( input, output size, forward pass)     \n",
        "# 2) Construct loss and optimizer.       l=((y_predicted - y)**2).mean()\n",
        "# 3) Training loop\n",
        "#     - Forward pass: compute prediction.   w * x        :Loss\n",
        "#     - Backward pass: gardient     l.backward\n",
        "#     - Update the weigths ( keep going in a loop untill the loss is minimized).    w - =lr *w.grad --> Optimizer "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PetXkLZng7OO",
        "colab_type": "text"
      },
      "source": [
        "using sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hqNT_ZsT-Qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn \n",
        "import numpy as np\n",
        "from sklearn import datasets \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# prepare data \n",
        "X_numpy, y_numpy = datasets.make_regression(n_samples= 100, n_features=1, noise=20, random_state= 1)\n",
        "\n",
        "#print(X_numpy,y_numpy)\n",
        "\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "\n",
        "y= y.view(-1 ,1 )\n",
        "\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# model\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "\n",
        "#loss function \n",
        "learning_rate = 0.01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "\n",
        "#training loop\n",
        "\n",
        "num_epochs = 500 \n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  #prediction = forward pass\n",
        "  y_pred = model(X)\n",
        "  #loss\n",
        "  loss = criterion( y_pred, y)\n",
        "\n",
        "  #gradients = backward pass\n",
        "  loss.backward() #loss with respect to w --> dl/dw\n",
        "\n",
        "  #update weights\n",
        "  optimizer.step()\n",
        "  #zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print(f'epoch {epoch+1}: loss =  {loss.item():.8f}')\n",
        "\n",
        " #plot \n",
        "predicted = model(X).detach().numpy()\n",
        "\n",
        "plt.plot(X_numpy,y_numpy,\"ro\")\n",
        "\n",
        "plt.plot(X_numpy, predicted, \"b\")\n",
        "\n",
        "plt.show()\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_scJe7rbiJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLFfz6vUgtF0",
        "colab_type": "text"
      },
      "source": [
        "#Logistic Regression "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNPCxpI3gzKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn \n",
        "import numpy as np\n",
        "from sklearn import datasets \n",
        "from sklearn.preprocessing import StandardScaler # \n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk8qLYWNheHC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ee9d75f6-ef82-4164-dc81-fc4a3da658de"
      },
      "source": [
        "bc = datasets.load_breast_cancer()\n",
        "\n",
        "X,y = bc.data, bc.target\n",
        "\n",
        "n_samples,n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size = 0.2, random_state = 1234)\n",
        "\n",
        "#scale\n",
        "\n",
        "sc = StandardScaler() #recommend to do \n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "\n",
        "#reshape \n",
        "\n",
        "y_train = y_train.view(-1,1)\n",
        "y_test = y_test.view(-1,1)\n",
        "\n",
        "#model \n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self,n_inputs_features):\n",
        "    super(LogisticRegression,self).__init__()\n",
        "    self.linear = nn.Linear(n_inputs_features, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_predicted = torch.sigmoid(self.linear(x))\n",
        "    return y_predicted \n",
        "\n",
        "model = LogisticRegression(n_features)    \n",
        "\n",
        "\n",
        "\n",
        "# loss and optimizer \n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss() # binary cross entopy \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "# training loop\n",
        "\n",
        "num_epochs = 30000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  #prediction = forward pass\n",
        "  y_predicted = model(X_train)\n",
        "  #loss\n",
        "  loss = criterion( y_predicted, y_train)\n",
        "\n",
        "  #gradients = backward pass\n",
        "  loss.backward() #loss with respect to w --> dl/dw\n",
        "\n",
        "  #update weights\n",
        "  optimizer.step()\n",
        "  #zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1) % 10000 == 0:\n",
        "    print(f'epoch {epoch+1}: loss =  {loss.item():.8f}')\n",
        "\n",
        "#evaluation \n",
        "with torch.no_grad():\n",
        "  y_predicted = model(X_test)\n",
        "  y_predicted_cls = y_predicted.round()  \n",
        "  acc = y_predicted_cls.eq(y_test).sum()/float(y_test.shape[0])  \n",
        "  print (f\"accuray = {acc:4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 10000: loss =  0.04072985\n",
            "epoch 20000: loss =  0.03326616\n",
            "epoch 30000: loss =  0.02980602\n",
            "accuray = 0.956140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh3CF87m0ba-",
        "colab_type": "text"
      },
      "source": [
        "#dataset and data loader "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-_j1rw20l3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4jWc42k28yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WineDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    xy = np.loadtxt(\"/wine.csv\", delimiter=\",\",dtype=np.float32, skiprows=1)\n",
        "    self.X = torch.from_numpy(xy[:, 1:])\n",
        "    self.y = torch.from_numpy(xy[:,[0]])\n",
        "    self.n_samples = xy.shape[0]\n",
        "    \n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.X[index],self.y[index]\n",
        "\n",
        "\n",
        "  def __len__(self): \n",
        "     return self.n_samples\n",
        "\n",
        "dataset = WineDataset()\n",
        "dataloader = DataLoader(dataset = dataset, batch_size = 4, shuffle=True,num_workers = 2)\n",
        "\n",
        "# dataiter = iter(dataloader)\n",
        "# data = dataiter.next()\n",
        "# features ,labels = data \n",
        "\n",
        "# print (features, labels)\n",
        "\n",
        "# training loop \n",
        "num_epochs = 2\n",
        "total_samples = len(dataset)\n",
        "n_iterations = math.ceil(total_samples/4)\n",
        "#print (total_samples, n_iterations)\n",
        "\n",
        "\n",
        "for epoch in range (num_epochs):\n",
        "  for i, (inputs,labels) in enumerate(dataloader):\n",
        "\n",
        "    if (i+1) % 5 ==0:\n",
        "      print (f\"epoch{epoch+1}/{num_epochs}, step {i+1}/{n_iterations},inputs{inputs.shape}\")\n",
        "\n",
        "torchvision.datasets.MNIST()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jZ8Hbr7Dd3I",
        "colab_type": "text"
      },
      "source": [
        "#data Transforms "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1U4l-2cDhwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "class WineDataset(Dataset):\n",
        "  def __init__(self, transform=None):\n",
        "    xy = np.loadtxt(\"/wine.csv\", delimiter=\",\",dtype=np.float32, skiprows=1)\n",
        "    self.X = torch.from_numpy(xy[:, 1:])\n",
        "    self.y = torch.from_numpy(xy[:,[0]])\n",
        "    self.n_samples = xy.shape[0]\n",
        "    \n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.X[index],self.y[index]\n",
        "\n",
        "\n",
        "  def __len__(self): \n",
        "     return self.n_samples\n",
        "\n",
        "dataset = WineDataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w765sMX53IhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "\n",
        "  def __init__(self, transform=None):\n",
        "    #data loading\n",
        "    xy = np.loadtxt(\"Wine.csv\", delimiter =\",\", dtype=np.float32)\n",
        "    self.x = (xy[:,1:])\n",
        "    self.y =(xy[:,[0]]) #n_samples, 1\n",
        "    self.n_samples =xy.shape[0]\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    #dataset[0]\n",
        "    sample = self.x[index], self.y[index]\n",
        "    if self.transform:\n",
        "      sample =self.transform(sample)\n",
        "\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    #len(dataset)\n",
        "    return self.n_samples\n",
        "\n",
        "\n",
        "class ToTensor:\n",
        "  def __call__(self, sample):\n",
        "    inputs, targets = sample\n",
        "    return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "\n",
        "\n",
        "class MulTransform:\n",
        "  def __init__(self, factor):\n",
        "    self.factor=factor\n",
        "\n",
        "  def __call__(self, sample):\n",
        "    inputs, targets = sample\n",
        "    inputs *= self.factor\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# dataset = WineDataset(transform=ToTensor())\n",
        "# first_data = dataset[0]\n",
        "# features, labels = first_data\n",
        "# print(first_data)\n",
        "\n",
        "\n",
        "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(2)])\n",
        "dataset =WineDataset(transform=composed)\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(first_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCYkazPP3mxL",
        "colab_type": "text"
      },
      "source": [
        "#softmax and cross-entropy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_47XOJO3sT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#softmax  logit is applied on the output to squish the output "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm1xjf9x4tTY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "07a44cbc-e4f5-45d0-edad-fe68b0b40a67"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# softmax function \n",
        "def softmax(x):\n",
        "  return np.exp(x)/np.sum(np.exp(x), axis=0)\n",
        "\n",
        "x = np.array([2.0,1.0,0.1])\n",
        "outputs = softmax(x)\n",
        "print(\"softmax numpy:\",outputs)  \n",
        "\n",
        "\n",
        "# SOFTmax with torch \n",
        "xt = torch.tensor([2.0,1.0,0.1])\n",
        "output = torch.softmax(xt,dim=0)\n",
        "print(\"softmax tensor :\",output)  \n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "softmax numpy: [0.65900114 0.24243297 0.09856589]\n",
            "softmax tensor : tensor([0.6590, 0.2424, 0.0986])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT7eOcv6HIc8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c52d3b59-e0fa-410e-f767-5048bda61d3e"
      },
      "source": [
        "# cross entropy \n",
        "\n",
        "def cross_entropy(actual,predicted):\n",
        "  loss = -np.sum(actual* np.log(predicted))\n",
        "  return loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y = np.array([1,0,0])\n",
        "\n",
        "y_pred_good = np.array([0.7,0.2,0.1])\n",
        "y_pred_bad = np.array([0.1,0.3,0.6])\n",
        "\n",
        "l1 = cross_entropy(y,y_pred_good)\n",
        "l2 = cross_entropy (y,y_pred_bad) \n",
        "print(f\"loss1 numpy:{l1:.4f}\")\n",
        "print(f\"loss2 numpy:{l2:.4f}\")\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss1 numpy:0.3567\n",
            "loss2 numpy:2.3026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzeFuRR3PtKi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2bd82d06-0789-46d7-b343-bc603189dd18"
      },
      "source": [
        "# loss \n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "y = torch.tensor([0])\n",
        "y_pred_good = torch.tensor([[2.0,1.0,0.1]])\n",
        "y_pred_bad = torch.tensor([[0.5,2.0,0.3]])\n",
        "\n",
        "l1 = loss(y_pred_good, y)\n",
        "l2 = loss(y_pred_bad,y) \n",
        "\n",
        "print(l1.item())\n",
        "print(l2.item())\n",
        "\n",
        "_, prediction1 = torch.max(y_pred_good,1)\n",
        "_, prediction2 = torch.max(y_pred_bad,1)\n",
        "\n",
        "print(prediction1)\n",
        "print(prediction2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4170299470424652\n",
            "1.840616226196289\n",
            "tensor([0])\n",
            "tensor([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL-MeVadWyL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "226ace16-1f4a-446f-c870-4490aca49e35"
      },
      "source": [
        "# with three labels \n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "y = torch.tensor([0, 1, 2])\n",
        "y_pred_good = torch.tensor([[2.0,1.0,0.1],[1.0,2.0,0.1],[0.1,1.0,2.1]])\n",
        "y_pred_bad = torch.tensor([[0.5,2.0,0.3],[2.5,2.0,0.3],[2.5,2.0,0.3]])\n",
        "\n",
        "l1 = loss(y_pred_good, y)\n",
        "l2 = loss(y_pred_bad,y) \n",
        "\n",
        "print(l1.item())\n",
        "print(l2.item())\n",
        "\n",
        "_, prediction1 = torch.max(y_pred_good,1)\n",
        "_, prediction2 = torch.max(y_pred_bad,1)\n",
        "\n",
        "print(prediction1)\n",
        "print(prediction2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.40603378415107727\n",
            "1.874053955078125\n",
            "tensor([0, 1, 2])\n",
            "tensor([1, 0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGIl3AeTY0di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# multi class \n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNet2(nn.Module):\n",
        "  def __init__(self,input_size, hidden_size,num_classes):\n",
        "    super(NeuralNet2, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu= nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.linear1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    # no softmax \n",
        "    return out\n",
        "\n",
        "model = NeuralNet2(input_size=28*28, hidden_size = 5, num_classes=3)\n",
        "criterion = nn.CrossEntropyLoss()       "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL7RaUwgZ69H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#binary calssification\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNet2(nn.Module):\n",
        "  def __init__(self,input_size, hidden_size):\n",
        "    super(NeuralNet2, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu= nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.linear1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "\n",
        "    y_pred = torch.sigmoid(out)\n",
        "\n",
        "    # no softmax \n",
        "    return y_pred \n",
        "\n",
        "model = NeuralNet2(input_size=28*28, hidden_size = 5)\n",
        "criterion = nn.CrossEntropyLoss()       "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQl9EkXrsizV",
        "colab_type": "text"
      },
      "source": [
        "#activation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoLk0svXsoMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# step fuction \n",
        "# sigmoid function (binary classification)\n",
        "# TanH function (-1 ,1 )\n",
        "# ReLU (non linear most populer)\n",
        "# Leaky ReLU (vanishing gradient problem)\n",
        "# softmax \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFueBUsQvQno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# binary classification using sigmoid \n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNet2(nn.Module):\n",
        "  def __init__(self,input_size, hidden_size):\n",
        "    super(NeuralNet2, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu= nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.linear1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    out = self.sigmoid(out)\n",
        "    return out \n",
        "\n",
        "    "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpT-0t4gwHAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# option 2 using torch \n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NeuralNet2(nn.Module):\n",
        "  def __init__(self,input_size, hidden_size):\n",
        "    super(NeuralNet2, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.linear2 = nn.Linear(hidden_size, 1)\n",
        "  def forward(self,x):\n",
        "    out = torch.relu(self.linear1(x))\n",
        "    out = torch.sigmoid(self.linear2(out))\n",
        "    return out \n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHDSBZUbz5_Z",
        "colab_type": "text"
      },
      "source": [
        "# Feed Forword Neural Network on MNIST Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTrVOYdlxf9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MNIST\n",
        "# DataLoader ,Transformatiom\n",
        "# Multilayer NeuralNet , Activation Function \n",
        "#loss and Optimizer \n",
        "#Training loop (batch training)\n",
        "# Model evaltion\n",
        "# GPU Support "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74DdJroy4QY7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "outputId": "515ec245-e1ef-40c6-9152-265737c9ea6e"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn \n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms \n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "\n",
        "# device config \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# hyper parameters \n",
        "input_size = 784  # 28*28\n",
        "hidden_size = 100\n",
        "num_classes = 10 \n",
        "num_epochs = 2\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "# MNIST dataset \n",
        "train_dataset = torchvision.datasets.MNIST(root='./data',train =True, transform= transforms.ToTensor(), download= True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data',train =False, transform= transforms.ToTensor())\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size= batch_size, shuffle= True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size= batch_size, shuffle= True)\n",
        "\n",
        "examples = iter(train_loader)\n",
        "samples, labels = examples.next()\n",
        "# print (samples.shape , labels.shape)\n",
        "\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.imshow(samples[i][0], cmap= \"gray\")\n",
        "# plt.show()  \n",
        "\n",
        "\n",
        "# nueral net \n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu= nn.ReLU()\n",
        "    self.linear2 = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    out = self.linear1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out) \n",
        "    return out  \n",
        "\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, num_classes)\n",
        "\n",
        "#loss and optimizer \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "# traning loop \n",
        "n_total_steps = len(train_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "    \n",
        "\n",
        "    images = images.reshape(-1, 28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "    # forward pass \n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    #backwords\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step() \n",
        "    if (i+1) % 100 ==0:\n",
        "       print(f\"epoch {epoch+1}/{num_epochs}, step{i+1}/{n_total_steps}, loss = {loss.item():.4f}\")\n",
        "\n",
        "# test \n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  for images , lables in test_loader :\n",
        "    images = images.reshape (-1, 28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "\n",
        "    #value, index \n",
        "    _, predictions = torch.max(outputs,1)\n",
        "    n_samples += lables.shape[0]\n",
        "    n_correct += (predictions == lables).sum().item()\n",
        "\n",
        "  acc = 100.0 *n_correct /n_samples\n",
        "  print(f\"accuracy={acc}\")  "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-021f0bae0c9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN0j-KBPNK5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vu2BKlWufp4",
        "colab_type": "text"
      },
      "source": [
        "# CNN on CIFAR 10 dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAwnQ6zVuicD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn \n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms \n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# device config \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "# hyper parameters \n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 4\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "\n",
        "# we transform thenm to normalize range [-1,1]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
        "\n",
        "\n",
        "# MNIST dataset \n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data',train =True, transform= transform, download= True)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data',train =False, transform= transform)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size= batch_size, shuffle= True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size= batch_size, shuffle= True)\n",
        "\n",
        "classes = (\"plane\",\"car\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\")\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3,6,5) #input size (RGB), output_size , kernal Size \n",
        "    self.pool= nn.MaxPool2d(2,2) #dim and stride \n",
        "    self.conv2 = nn.Conv2d(6,16,5)\n",
        "    self.fc1 = nn.Linear(16*5*5, 120) \n",
        "    self.fc2 = nn.Linear(120,84)\n",
        "    self.fc3 = nn.Linear(84,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = x.view(-1, 16*5*5)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        " \n",
        "\n",
        "model = ConvNet().to(device)\n",
        "\n",
        "#loss and optimizer \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "# traning loop \n",
        "n_total_steps = len(train_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    # forward pass \n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    #backwords\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step() \n",
        "    if (i+1) % 100 ==0:\n",
        "       print(f\"epoch {epoch+1}/{num_epochs}, step{i+1}/{n_total_steps}, loss = {loss.item():.4f}\")\n",
        "\n",
        "print(\"finish trainig\")\n",
        "\n",
        "\n",
        "\n",
        "# test \n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  n_class_correct = [0 for i in range(10)]\n",
        "  n_class_samples = [0 for i in range(10)]\n",
        "  for images , lables in test_loader :\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "    #value, index \n",
        "    _, predictions = torch.max(outputs,1)\n",
        "    n_samples += lables.shape[0]\n",
        "    n_correct += (predictions == labels).sum().item()\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      label = labels[i]\n",
        "      pred =predictions[i]\n",
        "      if (label == pred):\n",
        "        n_class_correct[label] += 1\n",
        "      n_class_samples[label] += 1\n",
        "\n",
        "  acc = 100.0 *n_correct /n_samples\n",
        "  print(f\"accuracy={acc}\") \n",
        "\n",
        "  # for i in range(10):\n",
        "  #   acc = 100.0 * n_class_correct[i]/n_class_samples[i]\n",
        "  #   print(f'Accuracy of {classes[i]}: {acc}%') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R-jSkr50LJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qn1HhLsJT8m",
        "colab_type": "text"
      },
      "source": [
        "#Transfer Learning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQqo_esW0PCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "d955e561c4e6415392ca5ac174c960f7",
            "82e1766955cc48628807032a252b911d",
            "440b9983388744ffb4a36209789be3f1",
            "e6b285f0f93b4f2da75529c3348c801c",
            "ac06841afe2a4d408a40b36575523422",
            "0fb038919eac4d74a76fbd89efa1b172",
            "e6a15f32a36f4d668d4bde961cc50d40",
            "4d537bf49f7a4f6ab2f76368ee1432d3"
          ]
        },
        "outputId": "84ec482e-60d3-4415-abcd-298f6b292cc6"
      },
      "source": [
        "from torchvision import models\n",
        "model = models.resnet18(pretrained = True)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d955e561c4e6415392ca5ac174c960f7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ_yI4g7PuiJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "464d8083-0826-4cce-a3e1-3c253ecc5305"
      },
      "source": [
        "for name, layer in model.named_modules():\n",
        "  print(name, layer)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n",
            "conv1 Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "bn1 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "relu ReLU(inplace=True)\n",
            "maxpool MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "layer1 Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "layer1.0 BasicBlock(\n",
            "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "layer1.0.conv1 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer1.0.bn1 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer1.0.relu ReLU(inplace=True)\n",
            "layer1.0.conv2 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer1.0.bn2 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer1.1 BasicBlock(\n",
            "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "layer1.1.conv1 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer1.1.bn1 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer1.1.relu ReLU(inplace=True)\n",
            "layer1.1.conv2 Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer1.1.bn2 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer2 Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "layer2.0 BasicBlock(\n",
            "  (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (downsample): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "layer2.0.conv1 Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "layer2.0.bn1 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer2.0.relu ReLU(inplace=True)\n",
            "layer2.0.conv2 Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer2.0.bn2 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer2.0.downsample Sequential(\n",
            "  (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "layer2.0.downsample.0 Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "layer2.0.downsample.1 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer2.1 BasicBlock(\n",
            "  (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "layer2.1.conv1 Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer2.1.bn1 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer2.1.relu ReLU(inplace=True)\n",
            "layer2.1.conv2 Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer2.1.bn2 BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer3 Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "layer3.0 BasicBlock(\n",
            "  (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (downsample): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "layer3.0.conv1 Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "layer3.0.bn1 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer3.0.relu ReLU(inplace=True)\n",
            "layer3.0.conv2 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer3.0.bn2 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer3.0.downsample Sequential(\n",
            "  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "layer3.0.downsample.0 Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "layer3.0.downsample.1 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer3.1 BasicBlock(\n",
            "  (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "layer3.1.conv1 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer3.1.bn1 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer3.1.relu ReLU(inplace=True)\n",
            "layer3.1.conv2 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer3.1.bn2 BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer4 Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "layer4.0 BasicBlock(\n",
            "  (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (downsample): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "layer4.0.conv1 Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "layer4.0.bn1 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer4.0.relu ReLU(inplace=True)\n",
            "layer4.0.conv2 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer4.0.bn2 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer4.0.downsample Sequential(\n",
            "  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "layer4.0.downsample.0 Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "layer4.0.downsample.1 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer4.1 BasicBlock(\n",
            "  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n",
            "layer4.1.conv1 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer4.1.bn1 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "layer4.1.relu ReLU(inplace=True)\n",
            "layer4.1.conv2 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "layer4.1.bn2 BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "avgpool AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "fc Linear(in_features=512, out_features=1000, bias=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmqOLoCjQR0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d7ddb1f-f32c-41b6-b564-c817df6a809b"
      },
      "source": [
        "model.fc "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=512, out_features=1000, bias=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hhKgB0qQajE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}